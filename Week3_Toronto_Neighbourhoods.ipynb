{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto Neighbourhood Segmentation Part 1\n",
    "\n",
    "##### In this notebook I have prepared the dataframe to be used with the more specific location data added in the second part. First I used BeautifulSoup to scrape the Wikipedia page and then I cleaned the data in order to obtain the appropriate dataframe for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.2)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11691 sha256=52efebf4687162430e8f204ce27a52fc4b17ed5985f9df28101db0b9a6b0b087\n",
      "  Stored in directory: C:\\Users\\Lenovo IPS540\\AppData\\Local\\pip\\Cache\\wheels\\87\\2a\\18\\4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (4.4.1)\n",
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: html5lib in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from html5lib) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lenovo ips540\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-cp37-none-any.whl size=1278 sha256=1d9f07b80c7ed17adda6f7469c05425dfa2c3becfaf52155efd30a2299a0f46b\n",
      "  Stored in directory: C:\\Users\\Lenovo IPS540\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Lenovo IPS540\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - geopy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    geographiclib-1.50         |             py_0          34 KB  conda-forge\n",
      "    geopy-2.1.0                |     pyhd3deb0d_0          64 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          98 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  geographiclib      conda-forge/noarch::geographiclib-1.50-py_0\n",
      "  geopy              conda-forge/noarch::geopy-2.1.0-pyhd3deb0d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "geopy-2.1.0          | 64 KB     |            |   0% \n",
      "geopy-2.1.0          | 64 KB     | ##5        |  25% \n",
      "geopy-2.1.0          | 64 KB     | ########## | 100% \n",
      "\n",
      "geographiclib-1.50   | 34 KB     |            |   0% \n",
      "geographiclib-1.50   | 34 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Install and import BeautifulSoup and lxml in order to be able to scrape the Wikipedia page\n",
    "!pip install wikipedia  \n",
    "!pip install lxml bs4 html5lib\n",
    "import wikipedia as wp\n",
    "import lxml\n",
    "\n",
    "import numpy as np   #librabry to handle data in a vectorized manner\n",
    "import pandas as pd    # library for data analsysis\n",
    "import json           # library to handle JSON files\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "!conda install -c conda-forge geopy --yes\n",
    "from geopy.geocoders import Nominatim    # convert an address into latitude and longitude values\n",
    "\n",
    "import requests   # library to handle requests\n",
    "from pandas.io.json import json_normalize   # tranform JSON file into a pandas dataframe\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans #import k means for clustering from scikit-learn\n",
    "\n",
    "import folium    # map rendering library\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>M1B</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Malvern, Rouge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>M1C</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Rouge Hill, Port Union, Highland Creek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M1E</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Guildwood, Morningside, West Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M1G</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Woburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M1H</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Cedarbrae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Postal Code      Borough                           Neighbourhood\n",
       "0         M1B  Scarborough                          Malvern, Rouge\n",
       "1         M1C  Scarborough  Rouge Hill, Port Union, Highland Creek\n",
       "2         M1E  Scarborough       Guildwood, Morningside, West Hill\n",
       "3         M1G  Scarborough                                  Woburn\n",
       "4         M1H  Scarborough                               Cedarbrae"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the wikipedia page and put it into a dataframe\n",
    "html = wp.page(\"List of postal codes of Canada: M\").html().encode(\"UTF-8\")\n",
    "df_toronto = pd.read_html(html)[0]\n",
    "\n",
    "# Remove cells where the borough is \"Not assigned\"\n",
    "df_toronto = df_toronto[df_toronto.Borough!= \"Not assigned\"]\n",
    "\n",
    "# Make sure neighbourhoods in the same postal code are listed together in one combined row\n",
    "df_toronto = df_toronto.groupby(['Postal Code', 'Borough'])['Neighbourhood'].apply(list).apply(lambda x:', '.join(x)).to_frame().reset_index()\n",
    "\n",
    "# If a neighbourhood is not assigned, set the neighbourhood value equal to the value of the borough\n",
    "for index, row in df_toronto.iterrows():\n",
    "    if row['Neighbourhood'] == 'Not assigned':\n",
    "        row['Neighbourhood'] = row['Borough']\n",
    "    \n",
    "\n",
    "#define the columns used\n",
    "columns= [\"Postal Code\", \"Borough\", \"Neighbourhood\"]\n",
    "\n",
    "\n",
    "df_toronto.head()   # display first 5 rows of the finished dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toronto.shape  # print the number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
