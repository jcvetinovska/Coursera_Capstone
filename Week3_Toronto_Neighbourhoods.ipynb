{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto Neighbourhood Segmentation Part 1\n",
    "\n",
    "##### In this notebook I have prepared the dataframe to be used with the more specific location data added in the second part. First I used BeautifulSoup to scrape the Wikipedia page and then I cleaned the data in order to obtain the appropriate dataframe for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from wikipedia) (2.25.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from beautifulsoup4->wikipedia) (2.2)\n",
      "Requirement already satisfied: lxml in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (4.6.2)\n",
      "Requirement already satisfied: bs4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: html5lib in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (0.9999999)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from html5lib) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from beautifulsoup4->bs4) (2.2)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ "
     ]
    }
   ],
   "source": [
    "# Install and import BeautifulSoup and lxml in order to be able to scrape the Wikipedia page\n",
    "!pip install wikipedia  \n",
    "!pip install lxml bs4 html5lib\n",
    "import wikipedia as wp\n",
    "import lxml\n",
    "\n",
    "import numpy as np   #librabry to handle data in a vectorized manner\n",
    "import pandas as pd    # library for data analsysis\n",
    "import json           # library to handle JSON files\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "!conda install -c conda-forge geopy --yes\n",
    "from geopy.geocoders import Nominatim    # convert an address into latitude and longitude values\n",
    "\n",
    "import requests   # library to handle requests\n",
    "from pandas.io.json import json_normalize   # tranform JSON file into a pandas dataframe\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans #import k means for clustering from scikit-learn\n",
    "\n",
    "import folium    # map rendering library\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the wikipedia page and put it into a dataframe\n",
    "html = wp.page(\"List of postal codes of Canada: M\").html().encode(\"UTF-8\")\n",
    "df_toronto = pd.read_html(html)[0]\n",
    "\n",
    "# Remove cells where the borough is \"Not assigned\"\n",
    "df_toronto = df_toronto[df_toronto.Borough!= \"Not assigned\"]\n",
    "\n",
    "# Make sure neighbourhoods in the same postal code are listed together in one combined row\n",
    "df_toronto = df_toronto.groupby(['Postal Code', 'Borough'])['Neighbourhood'].apply(list).apply(lambda x:', '.join(x)).to_frame().reset_index()\n",
    "\n",
    "# If a neighbourhood is not assigned, set the neighbourhood value equal to the value of the borough\n",
    "for index, row in df_toronto.iterrows():\n",
    "    if row['Neighbourhood'] == 'Not assigned':\n",
    "        row['Neighbourhood'] = row['Borough']\n",
    "    \n",
    "\n",
    "#define the columns used\n",
    "columns= [\"Postal Code\", \"Borough\", \"Neighbourhood\"]\n",
    "\n",
    "\n",
    "df_toronto.head()   # display first 5 rows of the finished dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toronto.shape  # print the number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
